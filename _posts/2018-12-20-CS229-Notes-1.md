---
layout: post
title: CS229 Notes 1
---

[TOC]

Let $x^{(i)}$ denote the input variables (or features), and $y^{(i)}$ denote the output or target variables. A training example is denoted by a pair $(x^{(i)},y^{(i)})$. The dataset which consists of $m$ training examples $\{ (x^{(i)},y^{(i)}) ; i=1,\dots,m \}$ is called the training set. The spaces of input and output are denoted by $\mathcal{X}$ and $\mathcal{Y}$ respectively. 

## Linear Regression

Let's say we want to approximate $y$ as a linear function of $x$ with $n$ features,
$$
h_\theta(x) = \theta_0 + \theta_1x_1 + \dots + \theta_nx_n
$$
Setting $x_0=1$ (intercept term), we can write the hypothesis in simplified vector form,
$$
h_\theta(x) = \sum_{i=0}^n \theta_ix_i = \boldsymbol\theta^T\mathbf{x}
$$
The cost function which describes how close $h(x)$ is to $y$ is defined as,
$$
J(\theta) = \frac{1}{2}\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2
$$
which leads to the ***ordinary least squares*** regression model.

### Least Mean Squares (LMS) Algorithm

To minimize $J(\theta)$, we can use the ***gradient descent*** algorithm, which starts with some initial $\theta$, and repeatedly performs the update (where $\alpha$ is called the ***learning rate***):
$$
\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta)
$$
(It is important to note this update is simultaneously performed for all values of $j=0,1,\dots,n$ in each iteration.)

For a single training example, the derivative of the cost function term is
$$
\begin{align}
\frac{\partial}{\partial\theta_j}J(\theta)
&= \frac{\partial}{\partial\theta_j} \frac{1}{2}(h_\theta(x)-y)^2 \\
&= 2\cdot\frac{1}{2}(h_\theta(x)-y) \cdot \frac{\partial}{\partial\theta_j}(h_\theta(x)-y) \\
&= (h_\theta(x)-y) \cdot \frac{\partial}{\partial\theta_j}\left(\sum_{i=0}^n\theta_ix_i-y\right) \\
&= (h_\theta(x)-y)x_j
\end{align}
$$
So the update rule becomes,
$$
\theta_j := \theta_j + \alpha (y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}
$$
which is called the LMS update rule (or Widrow-Hoff learning rule).

For a training set of $m$ examples, the algorithm can be derived as,
$$
\begin{align}
&\text{Repeat until convergence \{} \\
&\qquad\theta_j := \theta_j + \alpha\sum_{i=1}^m (y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}  &\text{(for every }j) \\
&\}
\end{align}
$$
This method looks at every example in the entire training set in each iteration, and is called ***batch gradient descent***.

The optimization problem we have posed for linear regression has only one global optima (no other local optima), so the gradient descent method will always converge (assuming the learning rate $\alpha$ is not too large).

The other way to find the optimal parameters is to consider one training example in each iteration,
$$
\begin{align}
&\text{Loop \{} \\
&\qquad\text{For } i=1 \text{ to } m, \text{\{} \\
&\qquad\qquad\theta_j := \theta_j + \alpha (y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}  &\text{(for every }j) \\
&\qquad\text{\}} \\
&\}
\end{align}
$$
This method is called ***stochastic gradient descent*** (or ***incremental gradient descent***), which can start making progress right away from each training example. This can be useful especially if $m$ is large, and often gets to the minimum much faster than batch gradient descent.

### The Normal Equation

To minimize $J(\theta)$, we can also directly taking its derivatives w.r.t. the $\theta_j$'s, and setting them to zero.

First some formulas for matrix derivatives,
$$
\begin{align}
\nabla_A \text{tr}AB &= B^T \tag{1} \\
\nabla_{A^T} f(A) &= (\nabla_A f(A))^T \tag{2} \\
\nabla_A \text{tr}ABA^{T}C &= CAB + C^{T}AB^{T} \tag{3} \\
\nabla_A|A| &= |A|(A^{-1})^T \tag{4} 
\end{align}
$$
Define the ***design matrix*** $X$ be the $m \times n$ matrix (in fact $m \times (n+1)$ if we include the intercept term) that contains the training examples' input values in its rows:
$$
X = \begin{bmatrix}
- & (x^{(1)})^T & - \\
- & (x^{(2)})^T & - \\
- & \vdots & - \\
- & (x^{(m)})^T & -
\end{bmatrix}
$$
And let $\vec{y}$ be the $m$-dimensional vector containing all the target values from the training set:
$$
\vec{y} = \begin{bmatrix}
y^{(1)} \\
y^{(2)}\\
\vdots \\
y^{(m)}
\end{bmatrix}
$$
We can easily verify that
$$
X\theta-\vec{y} = \begin{bmatrix}
h_\theta(x^{(1)}) - y^{(1)} \\
\vdots \\
h_\theta(x^{(m)}) - y^{(m)}
\end{bmatrix}
$$
so
$$
\begin{align}
\frac{1}{2} (X\theta-\vec{y})^T (X\theta-\vec{y})
&= \frac{1}{2} \Vert X\theta-\vec{y} \Vert_2^2 \\
&= \frac{1}{2} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 = J(\theta)
\end{align}
$$
Combining Equations (2) and (3) above, we get
$$
\begin{align}
\nabla_{A^T}\text{tr}ABA^TC &= (CAB+C^TAB^T)^T \\
&= B^TA^TC^T+BA^TC \tag{5}
\end{align}
$$


Taking derivatives of $J(\theta)$ w.r.t. $\theta$, we get
$$
\begin{align}
\nabla_\theta J(\theta) &= \nabla_\theta \frac{1}{2} (X\theta-\vec{y})^T (X\theta-\vec{y}) \\
&= \frac{1}{2} \nabla_\theta (\theta^TX^TX\theta - \theta^TX^T\vec{y} - \vec{y}^TX\theta + \vec{y}^T\vec{y}) \\
&= \frac{1}{2} \nabla_\theta \text{tr}(\theta^TX^TX\theta - \theta^TX^T\vec{y} - \vec{y}^TX\theta + \vec{y}^T\vec{y}) \\
&= \frac{1}{2} \nabla_\theta (\text{tr}\theta^TX^TX\theta - 2\text{tr}\vec{y}^TX\theta) \\
&= \frac{1}{2} (X^TX\theta + X^TX\theta - 2X^T\vec{y}) \\
&= X^TX\theta - X^T\vec{y}
\end{align}
$$

In the third step, we used the fact that the trace of a real number is just the real number itself; the fourth step used the fact that $\text{tr}A=\text{tr}A^T$; and the fifth step used Equation (5) with $A^T=\theta, B=B^T=X^TX$ and $C=I$ and Equation (1). 

To minimize $J(\theta)$, we set the derivatives to zero, and obtain the ***normal equations***:
$$
X^TX\theta=X^T\vec{y}
$$
So the value of $\theta$ that minimizes $J(\theta)$ is given in closed form,
$$
\theta = (X^TX)^{-1} X^T\vec{y}
$$

### Probabilistic Interpretation

Assume the target variables and the inputs are related via the equation,
$$
y^{(i)} = \theta^Tx^{(i)} + \epsilon^{(i)}
$$
where $\epsilon^{(i)}$ is an error term.

And assume the $\epsilon^{(i)}$ are IID (independently and identically distributed) according to a Gaussian distribution with mean zero and variance $\sigma^2$, written as $\epsilon^{(i)} \sim \mathcal{N}(0,\sigma^2)$.
$$
p(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{(\epsilon^{(i)})^2}{2\sigma^2}\right)
$$
This implies the distribution of $y^{(i)}$ given $x^{(i)}$ and parameterized by $\theta$ is
$$
p(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right)
$$
This distribution of $y^{(i)}$ can also be written as $y^{(i)}|x^{(i)};\theta\sim \mathcal{N}(\theta^Tx^{(i)},\sigma^2) $.

The log-likelihood (as a function of $\theta$ given the outcome of $X$ and $\vec{y}$) is
$$
\begin{align}
L(\theta) = L(\theta;X,\vec{y}) &= p(\vec{y}|X;\theta) \\
&= \prod_{i=1}^m p(y^{(i)}|x^{(i)};\theta) \\
&= \prod_{i=1}^m \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right) \\

\ell(\theta) = \log L(\theta) &= \sum_{i=1}^m \log\frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right) \\
&= m \log\frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{\sigma^2}\cdot\frac{1}{2} \sum_{i=1}^m (y^{(i)}-\theta^Tx^{(i)})^2
\end{align}
$$
To make the data as high probability as possible, we maximize the log-likelihood $\ell(\theta)$. This is the same as to minimize
$$
\frac{1}{2} \sum_{i=1}^m (y^{(i)}-\theta^Tx^{(i)})^2
$$
which is the familiar least-square cost function $J(\theta)$. (Note it does not depend on $\sigma^2$ value.)

So the least-squares regression corresponds to finding the maximum likelihood estimate of $\theta$, which is a very natural method for finding the best parameters.

### Locally Weighted Linear Regression

Assuming there is sufficient training data, the locally weighted linear regression makes the choice of features less critical.

In the original linear regression algorithm, to make a prediction at a query point $x$, we would:

1. Fit $\theta$ to minimize $\sum_{i} (y^{(i)}-\theta^Tx^{(i)})^2$
2. Output $\theta^Tx$

In contrast, the locally weighted linear regression (LWR) algorithm does the following:

1. Fit $\theta$ to minimize $\sum_{i} w^{(i)} (y^{(i)}-\theta^Tx^{(i)})^2$
2. Output $\theta^Tx$

where the $w^{(i)}$s are non-negative valued weights.

A standard choice for the weights is
$$
w^{(i)} = \exp\left( -\frac{(x^{(i)}-x)^2}{2\tau^2} \right)
$$
where $\tau$ controls how fast the weight of a training example falls off with distance to the query point, and is called the ***bandwidth*** parameter.

Note that the weights depend on the particular point $x$ at which we're trying to evaluate. Training examples close to the query point $x$ are given higher weights.

## Classification and Logistic Regression

### Logistic Regression

Let's choose the hypotheses $h_\theta(x)$ to be
$$
h_\theta(x) = g(\theta^Tx) = \frac{1}{1+e^{-\theta^Tx}}
$$
where
$$
g(z) = \frac{1}{1+e^{-z}}
$$
is called the ***logistic function*** or the ***sigmoid function***.

The derivative of the sigmoid function $g'$ is
$$
\begin{align}
g'(z) &= \frac{d}{dz} \frac{1}{1+e^{-z}} \\
&= \frac{1}{(1+e^{-z})^2} (e^{-z}) \\
&= \frac{1}{(1+e^{-z})} \cdot \left(1-\frac{1}{(1+e^{-z})}\right) \\
&= g(z)(1-g(z))
\end{align}
$$
Assume that
$$
\begin{align}
P(y=1 \mid x;\theta) &= h_\theta(x) \\
P(y=0 \mid x;\theta) &= 1-h_\theta(x)
\end{align}
$$
This can be written more compactly as
$$
p(y \mid x;\theta) = (h_\theta(x))^y (1-h_\theta(x))^{1-y}
$$
The likelihood is then
$$
\begin{align}
L(\theta) &= p(\vec{y} \mid X;\theta) \\
&= \prod_{i=1}^m p(y^{(i)} \mid x^{(i)};\theta) \\
&= \prod_{i=1}^m (h_\theta(x^{(i)}))^{y^{(i)}} (1-h_\theta(x^{(i)}))^{1-y^{(i)}}
\end{align}
$$
The log likelihood is thus
$$
\begin{align}
\ell(\theta) &= \log L(\theta) \\
&= \sum_{i=1}^m y^{(i)}\log h(x^{(i)}) + (1-y^{(i)})\log(1-h(x^{(i)}))
\end{align}
$$
To maximize this likelihood, we can use gradient ascent $\theta := \theta + \alpha\nabla_\theta\ell(\theta)$. Note the positive sign (rather than negative sign) as we are now maximizing the function.

For one training example $(x,y)$, the derivative would be:
$$
\begin{align}
\frac{\partial}{\partial\theta_j}\ell(\theta) &= \frac{\partial}{\partial\theta_j} ( y\log g(\theta^Tx)+(1-y)\log(1-g(\theta^Tx)) ) \\
&= \frac{\partial}{\partial g(\theta^Tx)} \Big( y\log g(\theta^Tx)+(1-y)\log(1-g(\theta^Tx)) \Big) \cdot \frac{\partial}{\partial\theta_j}g(\theta^Tx) \\
&= \left( y\frac{1}{g(\theta^Tx)} - (1-y)\frac{1}{1-g(\theta^Tx)} \right) \cdot g(\theta^Tx)(1-g(\theta^Tx)) \frac{\partial}{\partial\theta_j}\theta^Tx \\
& = \big( y(1-g(\theta^Tx))-(1-y)g(\theta^Tx) \big) x_j \\
&= (y-g(\theta^Tx)) x_j \\
&= (y-h_\theta(x)) x_j
\end{align}
$$
The stochastic gradient ascent rule is therefore
$$
\theta_j := \theta_j + \alpha(y^{(i)} - h_\theta(x^{(i)})) x_j^{(i)}
$$
Notice this formula looks identical to the LMS update rule, though the $h_\theta(x^{(i)})$ functions are different.

### Digression: The Perceptron Learning Algorithm

Consider a modified version of the logistic regression (threshold function),
$$
g(z) =
\begin{cases}
1 &\text{if } z \ge 0 \\
0 &\text{if } z \lt 0
\end{cases}
$$
If we let $h_\theta(x)=g(\theta^Tx)$ as before, and use the same update rule
$$
\theta_j := \theta_j + \alpha(y^{(i)} - h_\theta(x^{(i)})) x_j^{(i)}
$$
then we have the ***perceptron learning algorithm***.

It is argued to be a rough model for how individual neurons in the brain work.

### Another Algorithm for Maximizing $\ell(\theta)$

Newton's method for finding a zero of a function:
$$
\theta := \theta - \frac{f(\theta)}{f'(\theta)}
$$
The maxima of $\ell(\theta)$ corresponds to $\ell'(\theta)=0$. So to maximize $\ell(\theta)$, we can use the update rule:
$$
\theta := \theta - \frac{\ell'(\theta)}{\ell''(\theta)}
$$
As our $\theta$ is vector-valued, we generalize this method to multidimensional setting, which is
$$
\theta := \theta - H^{-1}\nabla_\theta \ell(\theta)
$$
where $\nabla_\theta\ell(\theta)$ is the usual vector of partial derivatives of $\ell(\theta)$ w.r.t the $\theta_i$'s, and $H$ is an $n \times n$ matrix (actually $(n+1) \times (n+1)$ if we include the intercept term) called the Hessian, whose entries are given by
$$
H_{ij} = \frac{\partial^2\ell(\theta)}{\partial\theta_i\partial\theta_j}
$$
Advantages of Newton's method:

* typically faster convergence than (batch) gradient descent, many fewer iterations.
* as long as $n$ is not too large, usually much faster overall.

Disadvantages of Newton's method:

* one iteration is more expensive (requires finding and inverting an $n \times n$ Hessian).

When Newton's method is used to maximize the logistic regression log likelihood function $\ell(\theta)$, the resulting method is also called ***Fisher scoring***.

## Generalized Linear Models

We've seen a regression example $y \mid x;\theta \sim \mathcal{N}(\mu,\sigma^2)$, and classification one $y \mid x;\theta \sim \text{Bernoulli}(\phi)$. We will show both are special cases of GLMs. 

### The Exponential Family

Exponential family distributions:
$$
p(y;\eta)=b(y) \exp(\eta^T T(y) - a(\eta)) \tag{6}
$$
$\eta$: ***natural parameter*** (also called the ***canonical parameter***) of the distribution.

$T(y)$: ***sufficient statistic*** (for our cases, it will often be $T(y)=y$).

$a(\eta)$: ***log partition function*** (so the distribution sums/integrates to 1).

A fixed choice of $T, a$ and $b$ defines a family (or set) of distributions that is parameterized by $\eta$.

For Bernoulli distribution,
$$
\begin{align}
p(y;\phi) &= \phi^y (1-\phi)^{(1-y)} \\
&= \exp(y\log\phi + (1-y)\log(1-\phi)) \\
&= \exp\left( \left(\log\left(\frac{\phi}{1-\phi}\right)\right)y + \log(1-\phi) \right)
\end{align}
$$
Here we can see $\eta=\log(\phi/(1-\phi))$. If we solve for $\phi$, we get $\phi=1/(1+e^{-n})$, which is the familiar sigmoid function!

So we have
$$
\begin{align}
b(y) &= 1 \\
\eta &= \log\left( \frac{\phi}{1-\phi} \right) \\
T(y) &= y \\
a(\eta) &= -\log(1-\phi) = \log(1+e^\eta)
\end{align}
$$
For Gaussian distribution, recall the value of $\sigma^2$ had no effect on our final choice of $\theta$. To simplify, let's set $\sigma^2=1$.
$$
\begin{align}
p(y;\mu) &= \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2}(y-\mu)^2\right) \\
&= \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2}y^2\right) \cdot \exp\left(\mu y-\frac{1}{2}\mu^2\right)
\end{align}
$$
So we have
$$
\begin{align}
b(y) &= \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2}y^2\right) \\
\eta &= \mu \\
T(y) &= y \\
a(\eta) &= \frac{1}{2}\mu^2 = \frac{1}{2}\eta^2
\end{align}
$$

### Constructing GLMs

Consider a classification or regression problem with the following three assumptions:

1. $y \mid x;\theta \sim \text{ExponentialFamily}(\eta)$
2. Our goal is to predict the expected value of $T(y)$ given $x$. In our cases $T(y)=y$, so we would like our prediction output to satisfy $h(x)=E[y|x]$.
3. $\eta$ and the inputs $x$ are linearly related, $\eta = \theta^Tx$. (Or if $\eta$ is vector-valued, then $\eta_i=\theta_i^Tx$.)

These three assumptions allow us to derive a very elegant class of learning algorithms, namely GLMs.

#### Ordinary Least Squares

To show that ordinary least squares is a special case of the GLM family of models, consider the target variable $y$ (also called the ***response variable*** in GLM terminology) is continuous, and we model $y \mid x;\theta \sim \mathcal{N}(\mu,\sigma^2)$ (Here $\mu$ may depend on $x$).

We have
$$
\begin{align}
h_\theta(x) &= E[y|x;\theta] & \text{(From Assumption 2)} \\
&= \mu & \text{(From Gaussian)}\\
&= \eta & \text{(From earlier derivation)}\\
&= \theta^Tx & \text{(From Assumption 3)}
\end{align}
$$

#### Logistic Regression

For logistic regression, since $y$ is binary-valued $y \in \{0,1\}$, consider $y \mid x;\theta \sim \text{Bernoulli}(\phi)$. So $E[y|x;\theta]=\phi$.

We have
$$
\begin{align}
h_\theta(x) &= E[y|x;\theta] & \text{(From Assumption 2)} \\
&= \phi & \text{(From Bernoulli)}\\
&= \frac{1}{1+e^{-\eta}} & \text{(From earlier derivation)}\\
&= \frac{1}{1+e^{-\theta^Tx}} & \text{(From Assumption 3)}
\end{align}
$$
Terminology: the function $g$ giving the distribution's mean as a function of the natural parameter ($g(\eta)=E[T(y);\eta]$) is called the ***canonical response function***. Its inverse $g^{-1}$ is called the ***canonical link function***.

So the canonical response function for the Gaussian family is just the identity function; and for the Bernoulli is the logistic function.

#### Softmax Regression

Consider a classification problem in which the response variable $y$ can take any one of $k$ values, so $y \in \{1,2,\dots,k\}$. The multinomial can be parameterized by $k-1$ parameters $\phi_1,\phi_2,\dots,\phi_{k-1}$, each representing the probability of each outcome $\phi_i=p(y=i;\phi)$. Note that it is not $k$ parameters since the probabilities sum up to 1, so $p(y=k;\phi)=1-\sum_{i=1}^{k-1}\phi_i$.

We define $T(y) \in \mathbb{R}^{k-1}$ as follows:
$$
T(1)=\begin{bmatrix}1 \\ 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix},
T(2)=\begin{bmatrix}0 \\ 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix},
T(3)=\begin{bmatrix}0 \\ 0 \\ 1 \\ \vdots \\ 0 \end{bmatrix},
\cdots,
T(k-1)=\begin{bmatrix}0 \\ 0 \\ 0 \\ \vdots \\ 1 \end{bmatrix},
T(k)=\begin{bmatrix}0 \\ 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix}
$$
Let $(T(y))_i$ denote the $i$-th element of the vector $T(y)$.

We define another indicator function $1\{.\}$ to take on a value of 1 if its argument is true, and 0 otherwise. So $(T(y))_i=1\{y=i\}$. Further, we know that $E[(T(y))_i]=p(y=i)=\phi_i$.

We then have
$$
\begin{align}
p(y;\phi) &= \phi_1^{1\{y=1\}} \phi_2^{1\{y=2\}} \dots \phi_{k-1}^{1\{y=k-1\}} \phi_k^{1\{y=k\}} \\
&= \phi_1^{1\{y=1\}} \phi_2^{1\{y=2\}} \dots \phi_{k-1}^{1\{y=k-1\}} \phi_k^{1-\sum_{i=1}^{k-1}1\{y=i\}} \\
&= \phi_1^{(T(y))_1} \phi_2^{(T(y))_2} \dots \phi_{k-1}^{(T(y))_{k-1}} \phi_k^{1-\sum_{i=1}^{k-1}(T(y))_i} \\
&= \exp((T(y))_1\log\phi_1 + (T(y))_2\log\phi_2 + \dots + (T(y))_{k-1}\log\phi_{k-1} ) \\
& \textstyle \qquad\qquad + \left(1-\sum_{i=1}^{k-1}(T(y))_i\right)\log\phi_k) \\
&= \exp((T(y))_1\log(\phi_1/\phi_k) + (T(y))_2\log(\phi_2/\phi_k) + \dots + (T(y))_{k-1}\log(\phi_{k-1}/\phi_k) \\
& \qquad\qquad + \log\phi_k) \\
&= b(y) \exp(\eta^T T(y) - a(\eta))
\end{align}
$$
where
$$
\begin{align}
b(y) &= 1 \\
\eta &= \begin{bmatrix}
  \log(\phi_1/\phi_k) \\
  \log(\phi_2/\phi_k) \\
  \vdots \\
  \log(\phi_{k-1}/\phi_k) \\
\end{bmatrix} \\
a(\eta) &= -\log\phi_k
\end{align}
$$
So in this case the multinomial is also an exponential family distribution.

The link function is given (for $i=1,\dots,k$) by
$$
\begin{align}
\eta_i = \log\frac{\phi_i}{\phi_k} && (\text{We define }\eta_k=\log(\phi_k/\phi_k)=0)
\end{align}
$$
The response function can be obtained by inverting the link function
$$
\begin{align}
e^{\eta_i} &= \frac{\phi_i}{\phi_k} \\
{\phi_k}e^{\eta_i} &= \phi_i \tag{7} \\
\Rightarrow{\phi_k}\sum_{i=1}^k e^{\eta_i} &= \sum_{i=1}^k\phi_i = 1
\end{align}
$$
Substitute $\phi_k=1/\sum_{i=1}^k e^{n_i}$ into Equation (7), we get the response function
$$
\phi_i = \frac{e^{\eta_i}}{\sum_{j=1}^k e^{\eta_j}}
$$
This function mapping from the $\eta$'s to the $\phi$'s is called the ***softmax*** function.

Let us use the same Assumption 3 (i.e. the $\eta_i$'s are linearly related to the $x$'s). So $\eta_i=\theta_i^Tx$ (for $i=1,\dots,k-1$) where $\theta_i \in \mathbb{R}^{n+1}$ are the parameters of the model. We also define $\theta_k=\mathbf{0}$ so that $\eta_k=\theta_k^Tx=0$, as given previously.

So
$$
\begin{align}
p(y=i|x,\theta) &= \phi_i \\
&= \frac{e^{\eta_i}}{\sum_{j=1}^k e^{\eta_j}} \\
&= \frac{e^{\theta_i^Tx}}{\sum_{j=1}^k e^{\theta_j^Tx}} \tag{8}
\end{align}
$$
This model is called ***softmax regression***. It is a generalization of logistic regression.

Our hypothesis output would be
$$
\begin{align}
h_\theta(x) &= E[T(y) \mid x;\theta] \\
&= E\left[ \left. \begin{matrix}
           1\{y=1\} \\
           1\{y=2\} \\
           \vdots \\
           1\{y=k-1\}
           \end{matrix} \right\vert x;\theta \right] \\
&= \begin{bmatrix}
   \phi_1 \\
   \phi_2 \\
   \vdots \\
   \phi_{k-1}
   \end{bmatrix} \\
&= \begin{bmatrix}
   \frac{\exp(\theta_1^Tx)}{\sum_{j=1}^k \exp(\theta_j^Tx)} \\
   \frac{\exp(\theta_2^Tx)}{\sum_{j=1}^k \exp(\theta_j^Tx)} \\
   \vdots \\
   \frac{\exp(\theta_{k-1}^Tx)}{\sum_{j=1}^k \exp(\theta_j^Tx)}
   \end{bmatrix}
\end{align}
$$
So our hypothesis will output the estimated probability $p(y=i|x;\theta)$ for every $i=1,\dots,k$. (Note even though $h_\theta(x) \in \mathbb{R}^{k-1}$, we can easily obtain $p(y=k|x;\theta) = 1 - \sum_{i=1}^{k-1}\phi_i$).

If we have a training set of $m$ examples $\{ (x^{(i)},y^{(i)}) ; i=1,\dots,m \}$ and would like to learn the parameters $\theta_i$'s. The log-likelihood is given by
$$
\begin{align}
\ell(\theta) &= \sum_{i=1}^m \log p(y^{(i)}|x{(i)};\theta) \\
&= \sum_{i=1}^m \log \prod_{l=1}^{k} \left( \frac{e^{\theta_l^Tx^{(i)}}}{\sum_{j=1}^k e^{\theta_j^Tx^{(i)}}} \right)^{1\{y^{(i)}=l\}}
& \text{(From Equation (8))}
\end{align}
$$
We can then find the maximum likelihood estimate of $\theta_i$'s by maximizing $\ell(\theta)$, using gradient ascent or Newton's method.