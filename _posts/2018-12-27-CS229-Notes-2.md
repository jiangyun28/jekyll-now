---
layout: post
title: CS229 Notes 2
---
Notes taken for CS229 lecture notes 2.

[TOC]

## Generative Learning Algorithm

Algorithms that try to model $p(y|x)$ (such as logistic regression) or map input space $\mathcal{X}$ to the labels $\{0,1\}$ (such as perceptron algorithm) are called ***discriminative*** learning algorithms.

Algorithms that instead try to model $p(x|y)$ (and $p(y)$) are called ***generative*** learning algorithms.

After modelling $p(y)$ (the ***class priors***) and $p(x|y)$, we can use Bayes rule to derive the posterior distribution on $y|x$:
$$
p(y|x) = \frac{p(x|y)p(y)}{p(x)}
$$
where $p(x)=p(x|y=1)p(y=1)+p(x|y=0)p(y=0)$.

In fact if we were to calculate $p(y|x)$ in order to make a prediction, we can ignore the denominator, since
$$
\begin{aligned}
\arg\max_y p(y|x) &= \arg\max_y \frac{p(x|y)p(y)}{p(x)} \\
                  &= \arg\max_y p(x|y)p(y)
\end{aligned}
$$

### Gaussian Discriminant Analysis

In Gaussian discriminant analysis (GDA), we assume that $p(x|y)$ is distributed according to a multivariate normal distribution.

#### The Multivariate Normal Distribution

The multivariate normal distribution in $n$-dimensions, is parameterized by a mean vector $\mu \in \mathbb{R}^n$ and a covariance matrix $\Sigma \in \mathbb{R}^{n \times n}$ where $\Sigma \ge 0$ is symmetric and positive semi-definite. Also written $\mathcal{N}(\mu,\Sigma)$, its density is given by:
$$
p(x;\mu,\Sigma) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} \exp\left( -\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu) \right)
$$
where $|\Sigma|$ denotes the determinant of the matrix $\Sigma$.

For random variable $X \sim \mathcal{N}(\mu,\Sigma)$, the mean is
$$
\operatorname{E}[X] = \int_x xp(x;\mu,\Sigma)dx = \mu
$$
And the covariance is
$$
\begin{align}
\operatorname{Cov}(X) &= \Sigma \\
&= \operatorname{E}[(X-\operatorname{E}[X])(X-\operatorname{E}[X])^T] \\
&= \operatorname{E}[XX^T] - (\operatorname{E}[X])(\operatorname{E}[X])^T
\end{align}
$$

#### The Gaussian Discriminant Analysis Model

When we have a classification problem in which the input features $x$ are continuous-valued random variables, we can use the GDA model as the following:
$$
\begin{align}
y &\sim \text{Bernoulli}(\phi) \\
x|y=0 &\sim \mathcal{N}(\mu_0,\Sigma) \\
x|y=1 &\sim \mathcal{N}(\mu_1,\Sigma)
\end{align}
$$

That is:

(Note we usually use the same covariance matrix $\Sigma$ though there are two different Gaussians.)
$$
\begin{align}
p(y) &= \phi^y (1-\phi)^{1-y} \\
p(x|y=0) &= \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} \exp\left( -\frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0) \right) \\
p(x|y=1) &= \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} \exp\left( -\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1) \right) \\
\end{align}
$$
The log-likelihood of the data is given by
$$
\begin{align}
\ell(\phi,\mu_0,\mu_1,\Sigma) &= \log\prod_{i=1}^m p(x^{(i)},y^{(i)};\phi,\mu_0,\mu_1,\Sigma) \\
&= \log\prod_{i=1}^m p(x^{(i)}|y^{(i)};\mu_0,\mu_1,\Sigma)p(y^{(i)};\phi) \\
&= \log\prod_{i=1}^m \left[\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} \exp\left( -\frac{1}{2}(x^{(i)}-\mu_0)^T\Sigma^{-1}(x^{(i)}-\mu_0) \right)\right]^{1-y^{(i)}} \\
& \qquad\qquad\cdot \left[\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} \exp\left( -\frac{1}{2}(x^{(i)}-\mu_1)^T\Sigma^{-1}(x^{(i)}-\mu_1) \right)\right]^{y^{(i)}}
\cdot \phi^{y^{(i)}} (1-\phi)^{1-y^{(i)}} \\
&= \sum_{i=1}^m\bigg( (1-y^{(i)}) \left[\log\left(\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\right)-\frac{1}{2}(x^{(i)}-\mu_0)^T\Sigma^{-1}(x^{(i)}-\mu_0) \right] \\
& \qquad\qquad + y^{(i)} \left[\log\left(\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\right)-\frac{1}{2}(x^{(i)}-\mu_1)^T\Sigma^{-1}(x^{(i)}-\mu_1) \right] \\
& \qquad\qquad + y^{(i)}\log\phi + (1-y^{(i)})\log(1-\phi) \bigg)
\end{align}
$$
To find the maximum likelihood estimates, we set the derivative w.r.t. the parameters to 0.

For $\phi$,
$$
\begin{align}
\frac{\partial\ell}{\partial\phi} &= \sum_{i=1}^m \left( y^{(i)}\frac{1}{\phi} - (1-y^{(i)})\frac{1}{1-\phi} \right) = 0 \\
&\Rightarrow \sum_{i=1}^m \left( y^{(i)}(1-\phi)-(1-y^{(i)})\phi \right) = 0 \\
&\Rightarrow \sum_{i=1}^m \left( y^{(i)}-\phi \right) = 0 \\
&\Rightarrow \sum_{i=1}^m y^{(i)} - m\phi = 0 \\
&\Rightarrow \phi = \frac{1}{m} \sum_{i=1}^m y^{(i)} = \frac{1}{m} \sum_{i=1}^m 1\{y^{(i)}=1\}
\end{align}
$$
For $\mu_0$,
$$
\begin{align}
\frac{\partial\ell}{\partial\mu_0} &= \sum_{i=1}^m  (1-y^{(i)})\left(-\frac{1}{2}\right)(-2\Sigma^{-1}(x^{(i)}-\mu_0)) \\
&= \sum_{i=1}^m (1-y^{(i)})\Sigma^{-1}(x^{(i)}-\mu_0) = 0 \\
&\Rightarrow \sum_{i=1}^m (1-y^{(i)})(x^{(i)}-\mu_0) = 0 \\
&\Rightarrow \sum_{i=1}^m (1-y^{(i)})x^{(i)} - \sum_{i=1}^m(1-y^{(i)})\mu_0 = 0 \\
&\Rightarrow \mu_0 = \frac{\sum_{i=1}^m(1-y^{(i)})x^{(i)}}{\sum_{i=1}^m(1-y^{(i)})} = \frac{\sum_{i=1}^m 1\{y^{(i)}=0\}x^{(i)}}{\sum_{i=1}^m 1\{y^{(i)}=0\}}
\end{align}
$$
Similarly for $\mu_1$,
$$
\begin{align}
\frac{\partial\ell}{\partial\mu_1} &= \sum_{i=1}^m  y^{(i)}\left(-\frac{1}{2}\right)(-2\Sigma^{-1}(x^{(i)}-\mu_1)) \\
&= \sum_{i=1}^m  y^{(i)}\Sigma^{-1}(x^{(i)}-\mu_1) = 0 \\
&\Rightarrow \sum_{i=1}^m y^{(i)}(x^{(i)}-\mu_1) = 0 \\
&\Rightarrow \sum_{i=1}^m y^{(i)}x^{(i)}- \sum_{i=1}^m y^{(i)}\mu_1 = 0 \\
&\Rightarrow \mu_1 = \frac{\sum_{i=1}^m y^{(i)}x^{(i)}}{\sum_{i=1}^m y^{(i)}} = \frac{\sum_{i=1}^m 1\{y^{(i)}=1\}x^{(i)}}{\sum_{i=1}^m 1\{y^{(i)}=1\}}
\end{align}
$$
For $\Sigma$,
$$
\begin{align}
\frac{\partial\ell}{\partial\Sigma} &= \sum_{i=1}^m\bigg[ (1-y^{(i)})\Big( -\frac{1}{2|\Sigma|}\cdot|\Sigma|\Sigma^{-T} - \frac{1}{2}(-\Sigma^{-T}(x^{(i)}-\mu_0)(x^{(i)}-\mu_0)^T\Sigma^{-T})\Big) \\
& \qquad\qquad\quad + y^{(i)}\Big( -\frac{1}{2|\Sigma|}\cdot|\Sigma|\Sigma^{-T} - \frac{1}{2}(-\Sigma^{-T}(x^{(i)}-\mu_1)(x^{(i)}-\mu_1)^T\Sigma^{-T})\Big) \bigg] \\
&= -\frac{1}{2}\Sigma^{-T} \sum_{i=1}^m \bigg[ (1-y^{(i)})\Big( 1 - (x^{(i)}-\mu_0)(x^{(i)}-\mu_0)^T\Sigma^{-T}\Big) \\
& \qquad\qquad\qquad\qquad + y^{(i)}\Big( 1 - (x^{(i)}-\mu_1)(x^{(i)}-\mu_1)^T\Sigma^{-T}\Big) \bigg] \\
& = -\frac{1}{2}\Sigma^{-T} \sum_{i=1}^m\left[ 1 - (x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T \Sigma^{-T}  \right] = 0 \\
&\Rightarrow \sum_{i=1}^m\left[ 1 - (x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T \Sigma^{-T}  \right] = 0 \\
&\Rightarrow m - \sum_{i=1}^m (x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T \Sigma^{-T} = 0 \\
&\Rightarrow m\Sigma - \sum_{i=1}^m (x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T = 0 \\
&\Rightarrow \Sigma = \frac{1}{m} \sum_{i=1}^m (x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T
\end{align}
$$
In the first equality, we used two formulas. First, the gradient of determinant $\frac{\partial}{\partial A}|A|=|A|A^{-T}$. The other $\frac{\partial}{\partial A}a^TA^{-1}b=-A^{-T}ab^TA^{-T}$where $A=\Sigma, a=b=x^{(i)}-\mu_k$.

In the second last equality, we multiply by $\Sigma$ and use the fact that $\Sigma^{-T}=\Sigma^{-1}$ (as $\Sigma$ is symmetric).

To summarize, the MLE of the parameters for GDA are:
$$
\begin{align}
\phi &= \frac{1}{m} \sum_{i=1}^m 1\{y^{(i)}=1\} \\
\mu_0 &= \frac{\sum_{i=1}^m 1\{y^{(i)}=0\}x^{(i)}}{\sum_{i=1}^m 1\{y^{(i)}=0\}} \\
\mu_1 &= \frac{\sum_{i=1}^m 1\{y^{(i)}=1\}x^{(i)}}{\sum_{i=1}^m 1\{y^{(i)}=1\}} \\
\Sigma &= \frac{1}{m} \sum_{i=1}^m (x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T
\end{align}
$$
If we plot a graph of $p(y=1|x)$, it will be in the shape of the logistic function. And we can use the decision boundary at which $p(y=1|x)=0.5$ to predict whether $y$ is in the positive or negative class.

#### Discussion: GDA and Logistic Regression

If we view the quantity $p(y=1|x;\phi,\mu_0,\mu_1,\Sigma)$ as a function of $x$, we will find that it can be expressed in the form of the logistic regression (with appropriate $\theta$ as a function of $\phi,\mu_0,\mu_1,\Sigma$). This is to say that if we assume $x|y \sim \text{Gaussian} \Rightarrow$ the posterior $p(y=1|x)$ is logistic. The converse, however, is not true. This shows that GDA makes stronger modelling assumption about the data than logistic regression.

There are many different assumptions (besides Gaussian) that can lead to $p(y|x)$ being logistic. For example, if we assume $p(x|y) \sim \text{ExponetialFamily}(\eta)$ (e.g. Poisson, Gamma etc), this will also imply $p(y=1|x)$ is logistic.

From the above, we can see the advantages and disadvantages of each:

* If the assumption $x|y \sim \text{Gaussian}$ is true (or approximately true), GDA will perform better. (In fact, it is asymptotically efficient. In other words, in the limit of very large training sets, no algorithm that is strictly better in terms of estimating $p(y|x)$). 

* GDA (by making stronger assumption) often requires less data (more data-efficient). 

* Logistic regression (by making less assumption) is more robust to model assumptions, though it requires slightly more data to fit. For this reason, in practice logistic regression is used more often than GDA.

### Naive Bayes

In GDA, the feature vectors $x$ were continuous, real-valued vectors. In Naive Bayes, the $x_j$'s are discrete-valued.

Consider an example of ***text classification*** problems, to classify whether an email is spam or not. For example, we choose a ***vocabulary*** of 50000 words (in practice typically mid-thousands to tens-thousands of words) and represent an email using a feature vector $x$. Each element $x_j$ represents whether the $j$-th word of the dictionary appears in the email. If the email contains the word, we set $x_j=1$; otherwise $x_j=0$.

To model $p(x|y)$, we make a very strong assumption called the ***Naive Bayes (NB) assumption***. We assume that the $x_j$'s are conditionally independent given $y$. The resulting algorithm is called the ***Naive Bayes classifier***.

So we have:
$$
\begin{align}
p(x_1,x_2,\dots,x_{50000}|y) &= p(x_1|y)p(x_2|y,x_1)p(x_3|y,x_1,x_2) \cdots p(x_{50000}|y,x_1,\dots,x_{49999}) \\
&= p(x_1|y)p(x_2|y)p(x_3|y) \cdots p(x_{50000}|y) \\
&= \prod_{j=1}^n p(x_j|y)
\end{align}
$$
The first equality uses the probability chain rule. The second equality used the NB assumption.

Our model parameters:
$$
\begin{align}
\phi_{j|y=1} &= p(x_j=1|y=1) \\
\phi_{j|y=0} &= p(x_j=1|y=0) \\
\phi_y &= p(y=1)
\end{align}
$$
The joint likelihood is given by, as usual:
$$
\mathcal{L}(\phi_y,\phi_{j|y=1},\phi_{j|y=0}) = \prod_{i=1}^m p(x^{(i)},y^{(i)})
$$
The maximum likelihood estimates can be derived as:
$$
\begin{align}
\phi_{j|y=1} &= \frac{\sum_{i=1}^m 1\{x_j^{(i)}=1 \and y^{(i)}=1\}}{\sum_{i=1}^m 1\{y^{(i)}=1\}} \\
\phi_{j|y=0} &= \frac{\sum_{i=1}^m 1\{x_j^{(i)}=1 \and y^{(i)}=0\}}{\sum_{i=1}^m 1\{y^{(i)}=0\}} \\
\phi_y &= \frac{\sum_{i=1}^m 1\{y^{(i)}=1\}}{m}
\end{align}
$$
Once we have the parameters, we can make a prediction based on:
$$
\begin{align}
p(y=1|x) &= \frac{p(x|y=1)p(y=1)}{p(x)} \\
&= \frac{\left(\prod_{j=1}^n p(x_j|y=1)\right) p(y=1)}{\left(\prod_{j=1}^n p(x_j|y=1)\right)p(y=1) + \left(\prod_{j=1}^n p(x_j|y=0)\right)p(y=0)}
\end{align}
$$
This can be generalized to where $x_j$ can take $k$ values instead of binary. We would simply model $p(x_j|y)$ as multinomial rather than as Bernoulli.

Even if some original inputs were continuously valued, it is quite common to discretize it. When the original continuous-valued features are not well-modelled by a multivariate normal distribution, discretizing the features and using Naive Bayes (instead of GDA) will often result in a better classifier.

#### Laplace Smoothing

The above algorithm will work fairly well, but there is a problem with it. Imagine you receive emails which contain a new word you have never seen before (assuming it is the 35000th word in the dictionary), i.e. $1\{x_{35000}^{(i)}=1\}$ is always zero in the training set.

So the Naive Bayes spam filter will then pick the MLE of the parameters to be:
$$
\begin{align}
\phi_{35000|y=1} &= \frac{\sum_{i=1}^m 1\{x_{35000}^{(i)}=1 \and y^{(i)}=1\}}{\sum_{i=1}^m 1\{y^{(i)}=1\}} = 0 \\
\phi_{35000|y=0} &= \frac{\sum_{i=1}^m 1\{x_{35000}^{(i)}=1 \and y^{(i)}=0\}}{\sum_{i=1}^m 1\{y^{(i)}=0\}} = 0
\end{align}
$$
Hence, when making prediction, both the numerator and denominator become zero (since $p(x_{35000}|y=1)=p(x_{35000}|y=0)=0$):
$$
\begin{align}
p(y=1|x) 
&= \frac{\left(\prod_{j=1}^n p(x_j|y=1)\right) p(y=1)}{\left(\prod_{j=1}^n p(x_j|y=1)\right)p(y=1) + \left(\prod_{j=1}^n p(x_j|y=0)\right)p(y=0)} \\
&= \frac{0}{0}
\end{align}
$$
To solve this problem, we can use ***Laplace smoothing***, which adds terms to both the numerator and denominator.

In the general form, for a multinomial random variable $z$ taking values in $\{1,\dots,k\}$. The maximum likelihood for $\phi_j=p(z=j)$ changes to the one on the right.
$$
\phi_j = \frac{\sum_{i=1}^m 1\{z^{(i)}=j\}}{m}
\quad\xrightarrow{\text{Laplace Smoothing}}\quad
\phi_j = \frac{\sum_{i=1}^m 1\{z^{(i)}=j\}+1}{m+k}
$$
Note that the sum of probabilities $\sum_{j=1}^k \phi_j=1$ still holds after Laplace smoothing. Also, $\phi_j \ne 0 \ \forall j$ which solves our problem.

As a result, in our Naive Bayes classifier, with Laplace smoothing, the new formula for the MLE of the parameters are:
$$
\begin{align}
\phi_{j|y=1} &= \frac{\sum_{i=1}^m 1\{x_j^{(i)}=1 \and y^{(i)}=1\}+1}{\sum_{i=1}^m 1\{y^{(i)}=1\}+2} \\
\phi_{j|y=0} &= \frac{\sum_{i=1}^m 1\{x_j^{(i)}=1 \and y^{(i)}=0\}+1}{\sum_{i=1}^m 1\{y^{(i)}=0\}+2}
\end{align}
$$
(In practice, we do not need to apply Laplace smoothing to $\phi_y$, since we will typically have fair fractions of both spam and non-spam emails in our training set. So $\phi_y$ will be a reasonable estimate of $p(y=1)$ and will not be zero.)

#### Event Models for Text Classification

In the context of text classification, the Naive Bayes uses the ***multi-variate Bernoulli event model***. In the model, we assume it is first randomly determined (according to $p(y)$) whether a spammer or non-spammer will send you the next email. Then the person decides whether to include each word in the dictionary inside the email according to the probability $p(x_j=1|y)=\phi_{j|y}$. Thus, the overall probability of a message is given by $p(y)\prod_{j=1}^n p(x_j|y)$.

There is another different model, called ***multinomial event model***. In this model, let $x_j$ denote the identity of the $j$-th word in the email. So $x_j$ is now an integer taking values in $\{1,2,\dots,|V|\}$, where $|V|$ is the size of our vocabulary (dictionary). An email of $n$ words is presented by a vector $(x_1,x_2,\dots,x_n)$ of length $n$; note that $n$ can be different for each training example.

In the multinomial event model, we assume that spam/non-spam is first randomly determined (according to $p(y)$) as before. Then the person first generates $x_1$ from some multinomial distribution over all words ($p(x_1|y)$), next generates $x_2$ independently from the same multinomial distribution, similarly for $x_3,x_4$ and so on. Thus, the overall probability of a message is given by $p(y)\prod_{j=1}^n p(x_j|y)$. Note this formula looks the same as in the previous multi-variate Bernoulli event model, but now $x_j|y$ is a multinomial, rather than a Bernoulli distribution.

The parameters for the multinomial event model are $\phi_y=p(y)$ as before, $\phi_{k|y=1}=p(x_j=k|y=1)$ (for any $j$) and $\phi_{k|y=1}=p(x_j=k|y=0)$ (for any $j$).

Hence the joint likelihood of the data is:
$$
\begin{align}
\mathcal{L}(\phi_y,\phi_{k|y=1},\phi_{k|y=0}) &= \prod_{i=1}^m p(x^{(i)},y^{(i)}) \\
&= \prod_{i=1}^m\left( \prod_{j=1}^{n_i} p(x_j^{(i)}|y;\phi_{k|y=1},\phi_{k|y=0}) \right) p(y^{(i)};\phi_y)
\end{align}
$$
The maximum likelihood of the parameters can be derived as:
$$
\begin{align}
\phi_{k|y=1} &= \frac{\sum_{i=1}^m \sum_{j=1}^{n_i} 1\{x_j^{(i)}=k \and y^{(i)}=1\}}{\sum_{i=1}^m 1\{y^{(i)}=1\} n_i} \\
\phi_{k|y=0} &= \frac{\sum_{i=1}^m \sum_{j=1}^{n_i} 1\{x_j^{(i)}=k \and y^{(i)}=0\}}{\sum_{i=1}^m 1\{y^{(i)}=0\} n_i} \\
\phi_y &= \frac{\sum_{i=1}^m 1\{y^{(i)}=1\}}{m}
\end{align}
$$
After Laplace smoothing, $\phi_{k|y=1}$ and $\phi_{k|y=0}$ become:
$$
\begin{align}
\phi_{k|y=1} &= \frac{\sum_{i=1}^m \sum_{j=1}^{n_i} 1\{x_j^{(i)}=k \and y^{(i)}=1\} + 1}  {\sum_{i=1}^m 1\{y^{(i)}=1\} n_i + |V|} \\
\phi_{k|y=0} &= \frac{\sum_{i=1}^m \sum_{j=1}^{n_i} 1\{x_j^{(i)}=k \and y^{(i)}=0\} + 1}  {\sum_{i=1}^m 1\{y^{(i)}=0\} n_i + |V|} \\
\end{align}
$$
While not necessarily the very best classification algorithm, the Naive Bayes classifier often works surprisingly well. It is often also a very good “first thing to try,” given its simplicity and ease of implementation.